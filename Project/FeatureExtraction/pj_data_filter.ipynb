{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ff9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# OS I/O\n",
    "import requests\n",
    "import inspect\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Image Processing\n",
    "import cv2\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir) \n",
    "\n",
    "from Utils._fe_utils import download_video_ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973d176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file gPO_sBM_c01_d10_mPO0_ch04.mp4: |██████████████| 100.0% Complete\n",
      "./gPO_sBM_c01_d10_mPO0_ch04.mp4\n"
     ]
    }
   ],
   "source": [
    "print(download_video_ffmpeg(\"https://aistdancedb.ongaaccel.jp/v1.0.0/video/2M/gPO_sBM_c01_d10_mPO0_ch04.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "class FFprobeError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "        \n",
    "\n",
    "def ffprobe(filename):\n",
    "    \"\"\"\n",
    "    Returns info about video/audio file using FFprobe.\n",
    "    Args:\n",
    "        filename (str): Path to the video file to measure.\n",
    "    Returns:\n",
    "        str: decoded FFprobe output (stdout) as one string.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    command = ['ffprobe', filename]\n",
    "    process = subprocess.Popen(\n",
    "        command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    try:\n",
    "        out, err = process.communicate(timeout=10)\n",
    "    except TimeoutExpired:\n",
    "        process.kill()\n",
    "        out, err = process.communicate()\n",
    "\n",
    "    if err:\n",
    "        raise FFprobeError(err)\n",
    "    else:\n",
    "        if out.splitlines()[-1].find(\"No such file or directory\") != -1:\n",
    "            raise FileNotFoundError(out.splitlines()[-1])\n",
    "        else:\n",
    "            return out\n",
    "        \n",
    "        \n",
    "def get_fps(filename):\n",
    "    \"\"\"\n",
    "    Gets the FPS (frames per second) value of a video using FFprobe.\n",
    "    Args:\n",
    "        filename (str): Path to the video file to measure.\n",
    "    Returns:\n",
    "        float: The FPS value of the input video file.\n",
    "    \"\"\"\n",
    "    out = ffprobe(filename)\n",
    "    out_array = out.splitlines()\n",
    "    video_stream = None\n",
    "    at_line = -1\n",
    "    while video_stream == None:\n",
    "        video_stream = out_array[at_line] if out_array[at_line].find(\n",
    "            \"Video:\") != -1 else None\n",
    "        at_line -= 1\n",
    "        if at_line < -len(out_array):\n",
    "            raise NoStreamError(\n",
    "                \"No video stream found. (Is this a video file?)\")\n",
    "    video_stream_array = video_stream.split(',')\n",
    "    fps = None\n",
    "    at_chunk = -1\n",
    "    while fps == None:\n",
    "        fps = float(video_stream_array[at_chunk].split(\n",
    "            ' ')[-2]) if video_stream_array[at_chunk].split(' ')[-1] == 'fps' else None\n",
    "        at_chunk -= 1\n",
    "        if at_chunk < -len(video_stream_array):\n",
    "            raise FFprobeError(\"Could not fetch FPS.\")\n",
    "    return fps\n",
    "        \n",
    "\n",
    "def get_framecount(filename, fast=True):\n",
    "    \"\"\"\n",
    "    Returns the number of frames in a video using FFprobe.\n",
    "    Args:\n",
    "        filename (str): Path to the video file to measure.\n",
    "    Returns:\n",
    "        int: The number of frames in the input video file.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    command_query_container = 'ffprobe -v error -select_streams v:0 -show_entries stream=nb_frames -of default=nokey=1:noprint_wrappers=1'.split(\n",
    "        ' ')\n",
    "    command_query_container.append(filename)\n",
    "    command_count = 'ffprobe -v error -count_frames -select_streams v:0 -show_entries stream=nb_read_frames -of default=nokey=1:noprint_wrappers=1'.split(\n",
    "        ' ')\n",
    "    command_count.append(filename)\n",
    "    command = command_query_container if fast else command_count\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    try:\n",
    "        out, err = process.communicate(timeout=10)\n",
    "    except TimeoutExpired:\n",
    "        process.kill()\n",
    "        out, err = process.communicate()\n",
    "\n",
    "    if err:\n",
    "        raise FFprobeError(err)\n",
    "\n",
    "    elif out:\n",
    "        if out.splitlines()[-1].find(\"No such file or directory\") != -1:\n",
    "            raise FileNotFoundError(out.splitlines()[-1])\n",
    "        elif out.startswith(\"N/A\"):\n",
    "            if fast:\n",
    "                return get_framecount(filename, fast=False)\n",
    "            else:\n",
    "                raise FFprobeError(\n",
    "                    \"Could not count frames. (Is this a video file?)\")\n",
    "        else:\n",
    "            return int(out)\n",
    "\n",
    "    else:\n",
    "        if fast:\n",
    "            return get_framecount(filename, fast=False)\n",
    "        else:\n",
    "            raise FFprobeError(\n",
    "                \"Could not count frames. (Is this a video file?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Motiongram(object):\n",
    "    \n",
    "    def __init__(self, path, offset=None, dur=None):\n",
    "        self.path = path\n",
    "        self.framecount = get_framecount(path)\n",
    "        self.fps = get_fps(path) \n",
    "        \n",
    "    def preprocess(video_frame):\n",
    "        pass\n",
    "\n",
    "    def motiongram(path):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video2numpy(path, process_func=None, size=None, sr=60.0, offset=None, dur=None, gray=False):\n",
    "    if not size:\n",
    "        sizex, sizey = 640, 480  \n",
    "    else:\n",
    "        sizex, sizey = size\n",
    "        \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    ret = True\n",
    "\n",
    "    frames_pro = []\n",
    "    frames_raw = []\n",
    "    frameCount = 0\n",
    "    frameSkipCount = 0\n",
    "    frames_offset  = int(dur * sr) if offset else int(0)\n",
    "    frames_to_load = int(dur * sr) if dur else int(1e+10)\n",
    "\n",
    "    last_frame = np.zeros((sizex, sizey)).T\n",
    "    \n",
    "    while ret and (frameSkipCount < frames_offset):\n",
    "        ret, frame = cap.read()\n",
    "        frameSkipCount += 1\n",
    "\n",
    "    while ret and (frameCount < frames_to_load):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            \n",
    "            # resize for faster processing\n",
    "            frame = cv2.resize(frame, (sizex, sizey))\n",
    "            \n",
    "            if gray:\n",
    "                # Use greyscale picture for faster processing\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                # Convert to standard rgb format (for plotting etc.)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            frames_raw.append(frame)\n",
    "            \n",
    "            if process_func:\n",
    "                # Process the frame (with custom function)\n",
    "                frame = process_func(frame=frame)\n",
    "                frames_pro.append(frame)\n",
    "                \n",
    "            frameCount += 1\n",
    "            \n",
    "    cap.release()\n",
    "    frames_out_raw = np.stack(frames_raw, axis=0)\n",
    "    frames_out_pro = np.stack(frames_pro, axis=0) if frames_pro else np.array([])\n",
    "    \n",
    "    output_dict = {\n",
    "        \"frames_raw\": frames_out_raw,\n",
    "        \"frames_processed\": frames_out_pro,\n",
    "        \"duration\": frameCount / sr,\n",
    "        \"sampleRate\": sr\n",
    "    }\n",
    "    \n",
    "    return frames_out_raw, frames_out_pro \n",
    "\n",
    "def findSignificantContour(edgeImg):\n",
    "    image, contours, hierarchy = cv2.findContours(\n",
    "        edgeImg,\n",
    "        cv2.RETR_TREE,\n",
    "        cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    \n",
    "    # Find level 1 contours\n",
    "    level1Meta = []\n",
    "    for contourIndex, tupl in enumerate(hierarchy[0]):\n",
    "        # Filter the ones without parent\n",
    "        if tupl[3] == -1:\n",
    "            tupl = np.insert(tupl.copy(), 0, [contourIndex])\n",
    "            level1Meta.append(tupl)\n",
    "    \n",
    "    # From among them, find the contours with large surface area.\n",
    "    contoursWithArea = []\n",
    "    for tupl in level1Meta:\n",
    "        contourIndex = tupl[0]\n",
    "        contour = contours[contourIndex]\n",
    "        area = cv2.contourArea(contour)\n",
    "        contoursWithArea.append([contour, area, contourIndex])\n",
    "    \n",
    "    contoursWithArea.sort(key=lambda meta: meta[1], reverse=True)\n",
    "    largestContour = contoursWithArea[0][0]\n",
    "    return largestContour\n",
    "\n",
    "def extractForeground(frame):\n",
    "    # Save a copy of the original frame\n",
    "    frame_orig = np.copy(frame)\n",
    "    \n",
    "    # Apply an adaptive threshold\n",
    "    frame = cv2.adaptiveThreshold(frame, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "                cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    \n",
    "    # Filter out salt-and-pepper noise\n",
    "    frame = cv2.medianBlur(frame, 5)\n",
    "    \n",
    "    # Use \"close\" morphological operation to close the gaps between contours\n",
    "    frame = cv2.morphologyEx(frame, cv2.MORPH_CLOSE,\\\n",
    "                cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)));\n",
    "    \n",
    "    # Find significant contour\n",
    "    contour = findSignificantContour(frame)\n",
    "    \n",
    "    # Create a mask from the contour\n",
    "    mask = np.zeros_like(frame)\n",
    "    cv2.fillPoly(mask, [contour], 255)\n",
    "    \n",
    "    # Use \"close\" morphological operation to close small\n",
    "    # gaps in the mask\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE,\\\n",
    "                cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)));\n",
    "    \n",
    "    # Mask out the bg\n",
    "    frame = cv2.bitwise_and(frame_orig, frame_orig, mask=mask)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def frame_process(*args):\n",
    "    # Extract foreground (dancer)\n",
    "    frame = args[0]\n",
    "    last_frame = args[1]\n",
    "    \n",
    "    frameFg = extractForeground(frame)\n",
    "    \n",
    "    # Blur the foreground image\n",
    "    frame = cv2.GaussianBlur(frameFg, (5, 5), 0)\n",
    "    \n",
    "    frame_diff = np.abs(frame - last_frame).astype(np.uint8)\n",
    "    last_frame = np.copy(frame)\n",
    "    \n",
    "    # Threshold again\n",
    "    _, frame = cv2.threshold(frame_diff, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Remove salt-and-pepper noise\n",
    "    frame = cv2.medianBlur(frame, 5)\n",
    "    \n",
    "    # Invert the image (visualisation)\n",
    "    frame = cv2.bitwise_not(frame)\n",
    "    \n",
    "    return frame, last_frame    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4febc",
   "metadata": {},
   "source": [
    "# CV TEST SCREEN (Creating MotionGrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relpath  = './../Data/'\n",
    "filename = 'refined_2M_all_video_url.csv'\n",
    "\n",
    "all_data = pd.read_csv(relpath + filename, header=None)\n",
    "data_mask = all_data[0].str.contains(\"c01\")\n",
    "\n",
    "urls = []\n",
    "for idx, url in enumerate(all_data[data_mask][0]):\n",
    "    urls.append(url)\n",
    "\n",
    "print(len(urls))\n",
    "test_url = urls[963]\n",
    "print(test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = Motiongram('./boomwhackers.mp4')\n",
    "print(mg.framecount, mg.fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb693b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video from test_url:\n",
    "test_url = './boomwhackers.mp4'\n",
    "frames_raw, frames_processed = video2numpy(\n",
    "    test_url, \n",
    "    process_func=None,\n",
    "    sr=24.0, \n",
    "    offset=15, \n",
    "    dur=60,\n",
    "    size=(320, 240)\n",
    ")\n",
    "\n",
    "print(frames_raw.shape, frames_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65868da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make motiongrams\n",
    "batchSize = 128\n",
    "batchStepsLo = np.array([i for i in range(0, frames_raw.shape[0], batchSize)])\n",
    "batchStepsHi = np.minimum(frames_raw.shape[0], batchStepsLo + batchSize)\n",
    "\n",
    "ft, fy, fx, fc = frames_raw.shape \n",
    "frame_diffs = np.zeros((ft-1, fy, fx, fc)).astype(np.float16)\n",
    "motiongram_x = np.zeros((fx, ft-1, fc)).astype(np.float16)\n",
    "motiongram_y = np.zeros((fy, ft-1, fc)).astype(np.float16)\n",
    "vertax = 1\n",
    "horzax = 2\n",
    "\n",
    "for lo, hi in zip(batchStepsLo, batchStepsHi):\n",
    "    #print(\"lo0: {0}, hi0: {1}\".format(lo, hi-1))\n",
    "    #print(\"lo1: {0}, hi1: {1}\".format(lo+1, hi))\n",
    "    frames0 = frames_raw[lo:hi-1].astype(np.float16)\n",
    "    frames1 = frames_raw[lo+1:hi].astype(np.float16)\n",
    "    fd = np.abs(frames1 - frames0)\n",
    "    \n",
    "    frame_diffs[lo:hi-1] = fd\n",
    "    \n",
    "    # split into rgb bands\n",
    "    for c in range(3):\n",
    "        ch = fd[:, :, :, int(c)]\n",
    "        chx_mu = np.mean(ch, axis=vertax) #np.mean(ch, axis=vertax)\n",
    "        chy_mu = np.mean(ch, axis=horzax) #np.mean(ch, axis=horzax)\n",
    "        motiongram_x[:, lo:hi-1, c] = chx_mu.T\n",
    "        motiongram_y[:, lo:hi-1, c] = chy_mu.T\n",
    "    \n",
    "frame_diffs = frame_diffs.astype(np.uint8)    \n",
    "motiongram_x = motiongram_x.astype(np.uint8)\n",
    "motiongram_y = motiongram_y.astype(np.uint8)\n",
    "\n",
    "#for i in range(3):\n",
    "#    motiongram_x[:, :, i] = motiongram_x[:, :, i] / np.amax(motiongram_x[:, :, i])\n",
    "#    motiongram_y[:, :, i] = motiongram_y[:, :, i] / np.amax(motiongram_y[:, :, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c86fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(motiongram_x.shape, motiongram_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0714b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(motiongram_y)\n",
    "plt.show()\n",
    "\n",
    "#thresh = 25\n",
    "#frame_diffs[frame_diffs > thresh] = 255\n",
    "#frame_diffs[frame_diffs <= thresh] = 0\n",
    "#frame_gray = frame_diffs[:, :, :, 0] * 0.33 + frame_diffs[:, :, :, 1] * 0.33 + frame_diffs[:, :, :, 2] * 0.34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "S_dB = librosa.power_to_db(frames_collapsed_x.T, ref=np.max)\n",
    "img = librosa.display.specshow(S_dB, x_axis='time',\n",
    "                         y_axis='mel', sr=60,\n",
    "                         fmax=8000, ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='Mel-frequency spectrogram')\n",
    "\n",
    "'''\n",
    "times = librosa.times_like(cent)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x_ = np.arange(frames_collapsed_x.shape[0]) / 60.0\n",
    "y_ = frames_collapsed_x[1] / np.amax(frames_collapsed_x[1])\n",
    "ax.pcolor(x_, y_, frames_collapsed_x.T, cmap='Spectral_r')\n",
    "ax.plot(x_, cent.T / np.amax(cent.T), label='Spectral centroid', color='w')\n",
    "ax.plot(x_, flat.T / np.amax(flat.T), label='Spectral flatness', color='k')\n",
    "ax.legend(loc='upper right')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff0125",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# open webcam video stream\n",
    "ret = True\n",
    "cap = cv2.VideoCapture(test_url)\n",
    "\n",
    "last_frame = np.zeros((640, 480)).T\n",
    "\n",
    "# Taking a matrix of size 5 as the kernel\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    " \n",
    "while(ret):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    \n",
    "    # resizing for faster detection\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "        \n",
    "    # using a greyscale picture, also for faster detection\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Save original gray-scale frame\n",
    "    frame_orig = np.copy(frame)\n",
    "    \n",
    "    # Apply an adaptive threshold\n",
    "    frame = cv2.adaptiveThreshold(frame, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "                cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    \n",
    "    frame = cv2.medianBlur(frame, 5)\n",
    "    \n",
    "    \n",
    "    # Use \"close\" morphological operation to close the gaps between contours\n",
    "    # https://stackoverflow.com/questions/18339988/implementing-imcloseim-se-in-opencv\n",
    "    frame = cv2.morphologyEx(frame, cv2.MORPH_CLOSE,\\\n",
    "                cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)));\n",
    "    \n",
    "    # Find contours\n",
    "    contour = findSignificantContour(frame)\n",
    "    \n",
    "    # Draw contours\n",
    "    #contourImg = np.copy(frame)\n",
    "    #cv2.drawContours(contourImg, [contour], 0, 127, 2, cv2.LINE_AA, maxLevel=1)\n",
    "    \n",
    "    \n",
    "    # Remove salt-and-pepper noise\n",
    "    frame = SaltPepperNoise(frame)\n",
    "    \n",
    "    # Use \"close\" morphological operation to close the gaps between contours\n",
    "    # https://stackoverflow.com/questions/18339988/implementing-imcloseim-se-in-opencv\n",
    "    frame = cv2.morphologyEx(frame, cv2.MORPH_CLOSE,\\\n",
    "                cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (31, 31)));\n",
    "    \n",
    "    # Find contours\n",
    "    contour = findSignificantContour(frame)\n",
    "    \n",
    "    # Draw contours\n",
    "    #contourImg = np.copy(frame)\n",
    "    #cv2.drawContours(contourImg, [contour], 0, 127, 2, cv2.LINE_AA, maxLevel=1)\n",
    "    \n",
    "    # Create a mask from the contour\n",
    "    mask = np.zeros_like(frame)\n",
    "    cv2.fillPoly(mask, [contour], 255)\n",
    "    \n",
    "    # Mask out the bg\n",
    "    frame = cv2.bitwise_and(frame_orig, frame_orig, mask=mask)\n",
    "    \n",
    "    frame_diff = np.abs(frame - last_frame).astype(np.uint8)\n",
    "    last_frame = np.copy(frame)\n",
    "    \n",
    "    _, frame = cv2.threshold(frame_diff, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    frame = SaltPepperNoise(frame)\n",
    "    \n",
    "    \n",
    "    # Create a mask from the contour\n",
    "    mask = np.zeros_like(frame)\n",
    "    cv2.fillPoly(mask, [contour], 255)\n",
    "    \n",
    "    # Use \"close\" morphological operation to close the gaps between contours\n",
    "    # https://stackoverflow.com/questions/18339988/implementing-imcloseim-se-in-opencv\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE,\\\n",
    "                cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)));\n",
    "    \n",
    "    # Mask out the bg\n",
    "    frame = cv2.bitwise_and(frame_orig, frame_orig, mask=mask)\n",
    "    \n",
    "    # Blur the masked image\n",
    "    frame = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "    \n",
    "    frame_diff = np.abs(frame - last_frame).astype(np.uint8)\n",
    "    last_frame = np.copy(frame)\n",
    "    \n",
    "    # Threshold again\n",
    "    _, frame = cv2.threshold(frame_diff, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Median filter again\n",
    "    frame = cv2.medianBlur(frame, 5)\n",
    "    \n",
    "    # Invert\n",
    "    frame = cv2.bitwise_not(frame)\n",
    "    \n",
    "    \n",
    "    # calculate sure foreground area by dilating the mask\n",
    "    #mapFg = cv2.erode(mask, np.ones((5, 5), np.uint8), iterations=10)    \n",
    "\n",
    "    # mark inital mask as \"probably background\"\n",
    "    # and mapFg as sure foreground\n",
    "    trimap = np.copy(mask)\n",
    "    trimap[mask  == 0]   = cv2.GC_BGD\n",
    "    trimap[mask  == 255] = cv2.GC_PR_BGD\n",
    "    trimap[mapFg == 255] = cv2.GC_FGD\n",
    "    \n",
    "    # visualize trimap\n",
    "    trimap_print = np.copy(trimap)\n",
    "    trimap_print[trimap_print == cv2.GC_PR_BGD] = 128\n",
    "    trimap_print[trimap_print == cv2.GC_FGD] = 255\n",
    "    \n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "\n",
    "# finally, close the window\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238991e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = \"http.//www.something.com/path1/path2.wav\"\n",
    "idx = h.rindex('/')\n",
    "print(h[idx+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed04aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
